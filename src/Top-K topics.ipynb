{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "from tika import parser\n",
    "\n",
    "# Pysaprk\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer as sp_cv\n",
    "from pyspark.ml.clustering import LDA, DistributedLDAModel\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "\n",
    "# NLTK\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store\n",
    "directory = '1987'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Annotation Platform\") \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data should be in the folder data\n",
    "filesRDD = spark.sparkContext.binaryFiles('data/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(filesRDD):\n",
    "    def tikanize_file(filename):\n",
    "        file_data = parser.from_file(filename)\n",
    "        text = file_data['content']\n",
    "        return text.lower()\n",
    "    \n",
    "    return filesRDD.map(lambda x: tikanize_file(x[0][5:]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filesContentRDD = process_files(filesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_documents = filesRDD.count()\n",
    "print('The number of files: ', number_of_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(filesRDD):\n",
    "    \n",
    "    # Extract and clean up text\n",
    "    def extract_and_clean_up(text):\n",
    "            end = min(text.find('acknowledgement'), text.find('references'), )\n",
    "            text = text[text.find('abstract')+9:end]\\\n",
    "            .replace('-\\n\\n', '').replace('\\n', ' ').replace('\\'', '')\n",
    "            return re.sub(r'[!@#$()Â©=\\+\\*:\\[\\]/0-9]{}', '', text) \n",
    "\n",
    "    # Tokenize\n",
    "    def tokenize(text):\n",
    "        result=[]\n",
    "        for token in gensim.utils.simple_preprocess(text, deacc=True) :\n",
    "            if token not in STOPWORDS and len(token) > 3:\n",
    "                result.append(token)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    return filesRDD.map(lambda x: tokenize(extract_and_clean_up(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanFilesRDD = clean_data(filesContentRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = cleanFilesRDD.collect()\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words)\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bingramDataRDD = cleanFilesRDD.map(lambda x : bigram_mod[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_data(dataRDD):\n",
    "    def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text))\n",
    "    \n",
    "    return dataRDD.map(lambda x : [lemmatize_stemming(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRDD = lemmatize_data(bingramDataRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform DataRDD to dataDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = dataRDD.zipWithIndex().map(lambda x: (x[1], x[0])).toDF(['index', 'words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# set minDF to 1 if the data folder contains just 1 file\n",
    "cv = sp_cv(inputCol=\"words\", outputCol=\"features\", minDF=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "modelCV = cv.fit(dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size\n",
    "initial_vocab = modelCV.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the initial vocab\n",
    "len(initial_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 5% of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold \n",
    "threshold = len(initial_vocab) / 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = modelCV.transform(dataDF)\\\n",
    "                .select('features').rdd\\\n",
    "                .flatMap(lambda x: list(zip(x[0].indices, x[0].values)))\\\n",
    "                .reduceByKey(lambda x, y: x + y).sortBy(lambda x: x[1]).zipWithIndex()\\\n",
    "                .filter(lambda x: threshold > x[1] or x[1] > len(initial_vocab) - threshold)\\\n",
    "                .map(lambda x: initial_vocab[x[0][0]]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the removed words\n",
    "len(words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"new_words\", stopWords=words_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dataDF = word_remover.transform(dataDF).select(['index', 'new_words'])\n",
    "clean_dataDF = clean_dataDF.select(col('index'), col(\"new_words\").alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "modelCV = cv.fit(clean_dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size\n",
    "clean_vocab = modelCV.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the clean final vocab\n",
    "len(clean_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_vectorized_dataDF = modelCV.transform(clean_dataDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select needed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldaDataDF = counter_vectorized_dataDF.select(['index', 'features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_grid(param_dict, dataDF, vocab, dataRDD):\n",
    "    params_coherence_list = []\n",
    "    count = 0\n",
    "    params_list = itertools.product(*param_dict.values())\n",
    "    for params in params_list: # (k, maxIter, optimizer, terms_per_topic)\n",
    "        count = count + 1\n",
    "        print('The number of iteration is ', count,'========>')\n",
    "        lda = LDA(k=params[0], maxIter=params[1], optimizer=params[2]) # Create LDA model\n",
    "        start = time.time()\n",
    "        ldaModel = lda.fit(dataDF) # Fit the data\n",
    "        end = time.time()\n",
    "        selected = ldaModel.describeTopics(maxTermsPerTopic = params[3]) # Select number of terms per topic\n",
    "        topics = selected.rdd.map(lambda x : (x[0], list(zip([vocab[i] for i in x[1]], x[2])))) # Get topics\n",
    "        coherence_topics = topics.map(lambda x : [i[0] for i in x[1]]) # Prepare data t oget coherence score\n",
    "        coherence_model_lda = gensim.models.CoherenceModel(topics=coherence_topics.collect(), texts=dataRDD.collect(), dictionary=Dictionary(dataRDD.collect()), coherence='c_v')\n",
    "        coherence_lda = coherence_model_lda.get_coherence()\n",
    "        params_coherence_list.append({'model':ldaModel, 'k': params[0], 'max_iteration': params[1], 'optimizer': params[2], 'terms_per_topic': params[3], 'time': end - start, 'coherence': coherence_lda})\n",
    "    return params_coherence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "k = [2, 3, 5, 10]\n",
    "maxIter = [20, 30, 50] \n",
    "optimizer = ['em']\n",
    "terms_per_topic = [5, 8]\n",
    "\n",
    "vocab = modelCV.vocabulary\n",
    "\n",
    "param_dict = {'k': k , 'maxIter' : maxIter, 'optimizer': optimizer, 'terms_per_topic': terms_per_topic}\n",
    "param_coherence_list = search_grid(param_dict, ldaDataDF, clean_vocab, clean_dataDF.rdd.map(lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to process\n",
    "print('Number of words to process ', clean_dataDF.rdd.flatMap(lambda x: x[1]).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model in param_coherence_list:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_n_models(models_list, n):\n",
    "    return sorted(models_list, key=lambda x:  -x.get('coherence'))[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the model and the number of words per topic, print the words in each topic\n",
    "def print_topics(model, number_of_words_per_topic, vocab):\n",
    "    selected = model.describeTopics(maxTermsPerTopic = number_of_words_per_topic)\n",
    "    topics = selected.rdd.map(lambda x : (x[0], list(zip([vocab[i] for i in x[1]], x[2]))))\n",
    "    for topic in topics.collect():\n",
    "        print('===Topic====', topic[0])\n",
    "        for word in topic[1]:\n",
    "            print('{: <20} {}'.format(word[0],word[1]))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best n models\n",
    "def save_models(models, path):\n",
    "    for model in models:\n",
    "        signature = str(model.get('k')) + '_' + str(model.get('max_iteration')) + '_' + str(model.get('terms_per_topic'))\n",
    "        model.get('model').save(path + '/'+signature)\n",
    "    print('All models have been saved in ', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best n models\n",
    "best = get_best_n_models(param_coherence_list, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for model in best:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print best model\n",
    "print_topics(best[1].get('model'), best[1].get('terms_per_topic'), clean_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save best n models\n",
    "#save_models(param_coherence_list, 'models/' + directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_csv(path, filename, models_info):\n",
    "    cols = ['model','k', 'max_iteration', 'optimizer', 'terms_per_topic', 'time', 'coherence']\n",
    "    try:\n",
    "        with open(path+'/'+filename, 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=cols)\n",
    "            writer.writeheader()\n",
    "            for data in models_info:\n",
    "                writer.writerow(data)\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store_to_csv('models/' + directory, 'data.csv', param_coherence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization: Remove some words from the seen topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "opt_cv = sp_cv(inputCol=\"words\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizing_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"new_words\", stopWords=irrelevant_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_dataDF = optimizing_remover.transform(clean_dataDF).select(['index', 'new_words'])\n",
    "optimized_dataDF = optimized_dataDF.select(col('index'), col(\"new_words\").alias(\"words\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "modelCV = opt_cv.fit(optimized_dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size\n",
    "optimized_vocab = modelCV.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_cv_dataDF = modelCV.transform(optimized_dataDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_ldaDataDF = optimized_cv_dataDF.select(['index', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = list(set([model.get('k') for model in best])) # 6\n",
    "maxIter = list(set([model.get('max_iteration') for model in best])) # 6\n",
    "optimizer = ['em']\n",
    "terms_per_topic = list(set([model.get('terms_per_topic') for model in best])) # 9\n",
    "\n",
    "param_dict = {'k': k , 'maxIter' : maxIter, 'optimizer': optimizer, 'terms_per_topic': terms_per_topic}\n",
    "opt_param_coherence_list = search_grid(param_dict, optimized_ldaDataDF, optimized_vocab, optimized_dataDF.rdd.map(lambda x : x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of words to process\n",
    "print('Number of words to process ', optimized_dataDF.rdd.flatMap(lambda x: x[1]).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in opt_param_coherence_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best n models\n",
    "opt_best = get_best_n_models(opt_param_coherence_list, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print best model\n",
    "print_topics(opt_best[0].get('model'), opt_best[0].get('terms_per_topic'), optimized_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# save best n models\n",
    "save_models(opt_best, 'models/' + directory + '/best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_to_csv('models/' + directory, 'best_data.csv', opt_param_coherence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
